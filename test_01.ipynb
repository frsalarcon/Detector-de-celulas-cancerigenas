{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPhiKjZ8u5rUUMNF+JgHpW/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frsalarcon/Detector-de-celulas-cancerigenas/blob/main/test_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2eqAtiGNFl4"
      },
      "source": [
        "# Modelo de prueba 1. CNN cancer de mamas. \r\n",
        "1000 imagenes de entrenamiento y  y 2000 de validacion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX28jImhNexb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0uc4K5bIaZd",
        "outputId": "b0b3dc79-78c6-4df3-db9c-a96ff0a06adc"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount ('/content/drive')\r\n",
        "import sys\r\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghLuXx6yJFRv"
      },
      "source": [
        "from tensorflow.keras import layers\r\n",
        " # Preprocesamiento de imagenes.\r\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\r\n",
        "# Optimizador (Adam)\r\n",
        "from tensorflow.python.keras import optimizers\r\n",
        "from tensorflow.keras import optimizers\r\n",
        "# Redes secuenciales\r\n",
        "from tensorflow.python.keras.models import Sequential\r\n",
        "# Capas: \r\n",
        "from tensorflow.python.keras.layers import Dropout, Flatten, Dense, Activation\r\n",
        "# Convolucion y maxpooling\r\n",
        "from tensorflow.python.keras.layers import  Convolution2D, MaxPooling2D\r\n",
        "# Para cerrar el backend de Keras\r\n",
        "from tensorflow.python.keras import backend as K"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLG9RSB6LKhC"
      },
      "source": [
        "# Ordenar data\r\n",
        "data_validacion='/content/drive/MyDrive/data/validacion'\r\n",
        "data_entrenamiento='/content/drive/MyDrive/data/entrenamiento'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEyDLVIPO3XL"
      },
      "source": [
        "Una tasa de aprendizaje mayor significa que el modelo podría alcanzar la pérdida mínima más rápido, pero también podría sobrepasar la mínima. Es más probable que las tasas de aprendizaje más pequeñas alcancen el mínimo, pero pueden llevar más tiempo. Por lo general, probamos tasas de aprendizaje entre 0,001 y 0,1 para encontrar la mejor para el entrenamiento de modelos. Puede establecer la tasa de aprendizaje a través del learning_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgILYm5XQ43S"
      },
      "source": [
        "batch_sizeestá el tamaño del lote de datos, decidido en tiempo de ejecución)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrgYjueOLTmB"
      },
      "source": [
        "\r\n",
        "epocas=20\r\n",
        "longitud, altura = 150, 150\r\n",
        "batch_size = 10\r\n",
        "pasos = 99\r\n",
        "validation_steps = 33\r\n",
        "filtrosconv1 = 32\r\n",
        "filtrosconv2 = 64\r\n",
        "tamaño_filtro1 = (3, 3)\r\n",
        "tamaño_filtro2 = (2, 2)\r\n",
        "tamaño_pool = (2, 2)\r\n",
        "clases = 2\r\n",
        "lr = 0.0004\r\n",
        "\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voO_7_50Ncrr"
      },
      "source": [
        "# Pre-procesamiento para entrenamiento\r\n",
        "# rescale= normalizacion, shear_range= inclinación , horizontal_flip=True  invertir imagen\r\n",
        "entrenamiento_datagen = ImageDataGenerator(rescale=1. / 255, shear_range=0.2, horizontal_flip=True)\r\n",
        "# # Pre-procesamiento para validación\r\n",
        "validacion_datagen=ImageDataGenerator( rescale=1./255)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xh2LVyCO-rA",
        "outputId": "44d66fd1-fbc2-42be-a79d-a5e288fa392c"
      },
      "source": [
        "# Training\r\n",
        "imagen_entrenamiento=entrenamiento_datagen.flow_from_directory(data_entrenamiento, target_size=(altura, longitud), batch_size=batch_size, class_mode='categorical')\r\n",
        "# Validate\r\n",
        "imagen_validacion=validacion_datagen.flow_from_directory(data_validacion, target_size=(altura, longitud), batch_size=batch_size, class_mode='categorical')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 999 images belonging to 2 classes.\n",
            "Found 2045 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOXMGtK3P1Os"
      },
      "source": [
        "En Keras, cada modelo de red neuronal es una instancia del Sequential.\r\n",
        "Esto actúa como el contenedor de la red neuronal, lo que nos permite construir el modelo apilando múltiples capas dentro del Sequentialobjeto.\r\n",
        "La capa de red neuronal de Keras más utilizada es la Densecapa. Esto representa una capa completamente conectada en la red neuronal y es el componente más importante de un modelo MLP.\r\n",
        "Al construir un modelo, comenzamos inicializando un Sequentialobjeto. Podemos inicializar un Sequentialobjeto vacío y agregar capas al modelo usando la addfunción, o podemos inicializar directamente el Sequentialobjeto con una lista de capas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBIa0uQbQqk0"
      },
      "source": [
        "La primera capa del Sequentialmodelo representa la capa de entrada. Por lo tanto, en la primera capa necesitamos especificar la dimensión de la característica de los datos de entrada para el modelo, lo que hacemos con el input_dimargumento de la palabra clave.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN_qdHUtPvL2"
      },
      "source": [
        "# CNN\r\n",
        "\r\n",
        "# Red neuronal secuencial capa a capa \r\n",
        "cnn=Sequential()\r\n",
        "# Primera capa. padding=, input_shape tamaño de la imagenes en la primera capa, relu sera la funcion de activacion.\r\n",
        "cnn.add(Convolution2D(filtrosconv1, tamaño_filtro1, padding =\"same\", input_shape=(longitud, altura, 3), activation='relu'))\r\n",
        "# Capa maxpooling\r\n",
        "cnn.add(MaxPooling2D(pool_size=tamaño_pool))\r\n",
        "\r\n",
        "\r\n",
        "# capa filtro 2.\r\n",
        "cnn.add(Convolution2D(filtrosconv2, tamaño_filtro2, padding='same', activation='relu'))\r\n",
        "# Capa maxpooling\r\n",
        "cnn.add(MaxPooling2D(pool_size=tamaño_pool))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTqhYGwtOAb7"
      },
      "source": [
        "Dense: toma una capa de neuronas y un tamaño de salida como argumentos requeridos y agrega una capa de salida completamente conectada con el tamañp dado al gráfico de cálculo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJLYcpEvQHJE"
      },
      "source": [
        "l Denseobjeto toma un único argumento requerido, que es el número de neuronas en la capa completamente conectada.\r\n",
        "activationargumento de palabra clave especifica la función de activación para la capa (el valor predeterminado es sin activación)\r\n",
        "\r\n",
        "ReLU – Rectified Lineal Unit\r\n",
        "La función ReLU transforma los valores introducidos anulando los valores negativos y dejando los positivos tal y como entran.\r\n",
        "\r\n",
        "Función ReLU\r\n",
        "Función ReLU\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "Características de la función ReLU:\r\n",
        "\r\n",
        "Activación Sparse – solo se activa si son positivos.\r\n",
        "No está acotada.\r\n",
        "Se pueden morir demasiadas neuronas.\r\n",
        "Se comporta bien con imágenes.\r\n",
        "Buen desempeño en redes convolucionales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThwHBii0RKdp"
      },
      "source": [
        "# Clasificación\r\n",
        "# Aplanar la imagen\r\n",
        "cnn.add(Flatten())\r\n",
        "cnn.add(Dense(256, activation='relu'))\r\n",
        "# Evitar el sobre ajuste apagando la mitad de las capas durante el train aleatoriamente, caminos alternos.\r\n",
        "cnn.add(Dropout(0.5))\r\n",
        "# Sofmax nos dice la probalidad en la ultima capa del resultado\r\n",
        "cnn.add(Dense(clases, activation='softmax'))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_R3Im-XOnLb"
      },
      "source": [
        "Un método de optimización popular y eficaz es Adam , que se implementa en TensorFlow como tf.train.AdamOptimizer. Tiene valores predeterminados ya establecidos para sus parámetros (por ejemplo learning_rate), por lo que en nuestro código inicializamos el objeto sin argumentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4gomGutRahb"
      },
      "source": [
        "Configuración para entrenamiento:\r\n",
        "Una sola llamada a la compilefunción permite configurar todos los requisitos de entrenamiento para el modelo.\r\n",
        "La función toma un solo argumento requerido, que es el optimizador a usar, por ejemplo, ADAM \r\n",
        "\r\n",
        "Los dos argumentos principales de palabras clave que debe conocer son lossy metrics. El lossargumento de la palabra clave especifica la función de pérdida que se utilizará. Para la clasificación binaria, establecemos el valor en 'binary_crossentropy', que es la función binaria de entropía cruzada. Para la clasificación multiclase, establecemos el valor en 'categorical_crossentropy', que es la función de entropía cruzada multiclase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZY4REcKSJE1"
      },
      "source": [
        "# optimizacion:\r\n",
        "cnn.compile(loss='categorical_crossentropy',\r\n",
        "            optimizer=optimizers.Adam(lr=lr),\r\n",
        "            metrics=['accuracy'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkIwX3C2PQcJ"
      },
      "source": [
        "parámetro de pérdida, una variable llamada loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUtj2qrUSLV8"
      },
      "source": [
        "Después de configurar un modelo de Keras para el entrenamiento, solo se necesita una línea de código para realizar el entrenamiento. Usamos la función Sequentialdel modelo fitpara entrenar el modelo en datos de entrada y etiquetas.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2F8M17gUqA7",
        "outputId": "896d1fdb-7f76-4cbf-9cdc-661b37ad8d75"
      },
      "source": [
        "cnn.fit(\r\n",
        "    imagen_entrenamiento,\r\n",
        "    steps_per_epoch=pasos,\r\n",
        "    epochs=epocas,\r\n",
        "    validation_data=imagen_validacion,\r\n",
        "    validation_steps=300)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.6560 - accuracy: 0.5898WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 184s 2s/step - loss: 0.6558 - accuracy: 0.5900 - val_loss: 0.6408 - val_accuracy: 0.6289\n",
            "Epoch 2/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.6268 - accuracy: 0.6434WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 46s 464ms/step - loss: 0.6269 - accuracy: 0.6433 - val_loss: 0.6230 - val_accuracy: 0.7702\n",
            "Epoch 3/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.6020 - accuracy: 0.6603WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 45s 460ms/step - loss: 0.6016 - accuracy: 0.6607 - val_loss: 0.5933 - val_accuracy: 0.6557\n",
            "Epoch 4/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.6159 - accuracy: 0.6596WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 46s 466ms/step - loss: 0.6158 - accuracy: 0.6596 - val_loss: 0.6405 - val_accuracy: 0.6303\n",
            "Epoch 5/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.5723 - accuracy: 0.6907WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 45s 461ms/step - loss: 0.5720 - accuracy: 0.6910 - val_loss: 0.5721 - val_accuracy: 0.6533\n",
            "Epoch 6/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.5332 - accuracy: 0.7273WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 45s 460ms/step - loss: 0.5332 - accuracy: 0.7274 - val_loss: 0.5614 - val_accuracy: 0.7477\n",
            "Epoch 7/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.5426 - accuracy: 0.7401WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 45s 461ms/step - loss: 0.5425 - accuracy: 0.7401 - val_loss: 0.7443 - val_accuracy: 0.5814\n",
            "Epoch 8/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.7014WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 45s 458ms/step - loss: 0.5583 - accuracy: 0.7016 - val_loss: 0.5869 - val_accuracy: 0.7457\n",
            "Epoch 9/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.4993 - accuracy: 0.7795WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 46s 464ms/step - loss: 0.4993 - accuracy: 0.7795 - val_loss: 0.6157 - val_accuracy: 0.7741\n",
            "Epoch 10/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.4842 - accuracy: 0.7680WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 45s 457ms/step - loss: 0.4841 - accuracy: 0.7680 - val_loss: 0.5710 - val_accuracy: 0.7232\n",
            "Epoch 11/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.7775WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 46s 463ms/step - loss: 0.4515 - accuracy: 0.7775 - val_loss: 0.5596 - val_accuracy: 0.7213\n",
            "Epoch 12/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.4708 - accuracy: 0.7636WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 45s 455ms/step - loss: 0.4707 - accuracy: 0.7638 - val_loss: 0.5172 - val_accuracy: 0.7848\n",
            "Epoch 13/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.4208 - accuracy: 0.8209WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 45s 461ms/step - loss: 0.4209 - accuracy: 0.8208 - val_loss: 0.6221 - val_accuracy: 0.7716\n",
            "Epoch 14/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.8095WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 46s 464ms/step - loss: 0.4253 - accuracy: 0.8095 - val_loss: 0.5492 - val_accuracy: 0.7746\n",
            "Epoch 15/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.8390WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 46s 472ms/step - loss: 0.3668 - accuracy: 0.8390 - val_loss: 0.6128 - val_accuracy: 0.7076\n",
            "Epoch 16/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.4143 - accuracy: 0.8093WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 46s 466ms/step - loss: 0.4142 - accuracy: 0.8093 - val_loss: 0.5484 - val_accuracy: 0.7863\n",
            "Epoch 17/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.3560 - accuracy: 0.8461WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 45s 461ms/step - loss: 0.3562 - accuracy: 0.8459 - val_loss: 0.5263 - val_accuracy: 0.7795\n",
            "Epoch 18/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.3173 - accuracy: 0.8695WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 46s 466ms/step - loss: 0.3175 - accuracy: 0.8693 - val_loss: 0.5814 - val_accuracy: 0.7589\n",
            "Epoch 19/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.3780 - accuracy: 0.8357WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 45s 456ms/step - loss: 0.3783 - accuracy: 0.8355 - val_loss: 0.5542 - val_accuracy: 0.7907\n",
            "Epoch 20/20\n",
            "99/99 [==============================] - ETA: 0s - loss: 0.3917 - accuracy: 0.8300WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n",
            "99/99 [==============================] - 45s 458ms/step - loss: 0.3915 - accuracy: 0.8301 - val_loss: 0.7396 - val_accuracy: 0.6680\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc974332ed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0_v1AOsVNaX"
      },
      "source": [
        "target_dir = './modelo1/'\r\n",
        "if not os.path.exists(target_dir):\r\n",
        "  os.mkdir(target_dir)\r\n",
        "cnn.save('/content/modelo1/modelo1.h5')\r\n",
        "cnn.save_weights('/content/modelo1/pesos1.h5')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5NiMSx62_9P"
      },
      "source": [
        "import numpy as np\r\n",
        "from keras.preprocessing.image import load_img, img_to_array\r\n",
        "from keras.models import load_model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfoG1B5333QN"
      },
      "source": [
        "longitud, altura = 150, 150\r\n",
        "modelo = '/content/modelo1/modelo1.h5'\r\n",
        "pesos_modelo = '/content/modelo1/pesos1.h5'\r\n",
        "cnn = load_model(modelo)\r\n",
        "cnn.load_weights(pesos_modelo)\r\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQOIPMZ2e7TJ"
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCwY9HAipN9N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a1363aba-56b6-44f8-bdc2-699568958f3d"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.download('/content/modelo1/modelo1.h5')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6993cf98-b244-46f0-98df-b02e895547d0\", \"modelo1.h5\", 269318480)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pwqqemcBfsEB",
        "outputId": "2f1fd7d7-c9f3-4a24-b942-6769ac93055e"
      },
      "source": [
        "files.download('/content/modelo1/pesos1.h5')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_fce1dce6-e3fd-41fe-beee-c2429c0ba9a4\", \"pesos1.h5\", 89779272)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaeI_BwGjauF"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zI9bpQWfxBn"
      },
      "source": [
        "# Prueba y predicción"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nED1H4YY4L9s"
      },
      "source": [
        "def predict(file):\r\n",
        "  x = load_img(file, target_size=(longitud, altura))\r\n",
        "  x = img_to_array(x)\r\n",
        "  x = np.expand_dims(x, axis=0)\r\n",
        "  array = cnn.predict(x)\r\n",
        "  result = array[0]\r\n",
        "  answer = np.argmax(result)\r\n",
        "  if answer == 0:\r\n",
        "    \r\n",
        "    print(\"pred:malign \")\r\n",
        "\r\n",
        "  elif answer == 1:\r\n",
        "    print(\"pred: bening\")\r\n",
        "\r\n",
        "\r\n",
        "  return answer"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KLIU5tfBSWU",
        "outputId": "2597b69b-fa02-420c-bdbc-7921b600c6d9"
      },
      "source": [
        "predict('/content/modelo1/10279_idx5_x201_y1051_class1.png')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred:malign \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGrLXk6sovmn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK_Zk4WhBfsI"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "bSXhFeVonT26",
        "outputId": "94f1ea09-6f12-4bec-d917-0ef788c753ea"
      },
      "source": [
        "La salida de la predictfunción es la salida del modelo. Eso significa que para la clasificación, la predictfunción son las probabilidades de clase del modelo para cada observación de datos."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-ea0e592f7a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    426\u001b[0m                   \u001b[0;34m'will be removed after 2021-01-01. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m                   'Please use `model.predict()` instead.')\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m       logging.warning('Network returning invalid probability values. '\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1097\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 964\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    965\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     raise RuntimeError(\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'function'>, <class 'NoneType'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv-9u_ylnnz4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}